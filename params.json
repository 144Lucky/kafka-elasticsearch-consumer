{"name":"Kafka-elasticsearch-consumer","tagline":"This project, Kafka Standalone Consumer will read the messages from Kafka, processes and index them in ElasticSearch.","body":"# Welcome to the kafka-elasticsearch-consumer wiki!\r\n\r\n## Illustration of kafka-elasticsearch-consumer usage\r\n\r\n### The consumer is positioned in the middle.\r\n\r\n![](https://raw.githubusercontent.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer/master/img/Kafka_ES_Illustration_New.png)\r\n\r\n\r\n# Introduction\r\n\r\n### **Kafka Standalone Consumer will read the messages from Kafka, processes and index them in ElasticSearch.**\r\n\r\n### **Easily Scaleable & Extendable !**\r\n\r\n### _As described in the illustration above, here is how the StandAlone Consumer works:_\r\n\r\n* Kafka has a topic named, say `Topic_1`\r\n\r\n* Lets say, `Topic_1` has 5 partitions.\r\n\r\n* Now, there is a needed to read, process the messages from Kafka and ElasticSearch\r\n\r\n* In order to do that, have 5 Config Files and start 5 instances of this Standalone Consumer by tying each config file to the respective Consumer Instance.\r\n\r\n* Now, we will have 5 Consumer Standalone Daemons running, listening & processing messages from each partition of `Topic_1` in Kafka.\r\n\r\n* When there is a new partitions(say `6th partition`) in the same `Topic_1`, then start a new Consumer Daemon instance pointing to the new partition(`i.e: 6th partition`)\r\n\r\n* This way, there is a clear way of subscribing and processing messages from multiple partitions across multiple topics using this Stand alone Consumer.\r\n\r\n# How to use ? \r\n\r\n### Method 1: running as a standard Jar \r\n\r\n**1. Download the code into a `$CONSUMER_HOME` dir.\r\n\r\n**2. cp `$CONSUMER_HOME`/src/main/resources/kafkaESConsumer.properties.template /your/absolute/path/kafkaESConsumer.properties file - update all relevant properties as explained in the comments\r\n\r\n**3. cp `$CONSUMER_HOME`/src/main/resources/logback.xml.template /your/absolute/path/logback.xml\r\n\r\n specify directory you want to store logs in:\r\n\t<property name=\"LOG_DIR\" value=\"/tmp\"/>\r\n\t\r\n adjust values of max sizes and number of log files as needed\r\n\r\n**4. build/create the app jar:\r\n\r\n\t\tcd $CONSUMER_HOME\r\n     \tmvn clean package\r\n     \t\r\n\tThe kafka-es-consumer-0.2.jar will be created in the $CONSUMER_HOME/bin, with all dependencies included into the JAR\r\n\r\n**5. run the app [use JDK1.8] :  \r\n\r\n\t\tjava -Dlogback.configurationFile=/your/absolute/path/logback.xml -jar $CONSUMER_HOME/bin/kafka-es-consumer-0.2.jar /your/absolute/path/kafkaESConsumer.properties\r\n\r\n \r\n\r\n### Method 2: running via JSVC as a Daemon\r\n\r\n**1. Download the code. Let's say, `$CONSUMER_HOME` contains the code.**\r\n\r\n**2. From the `$CONSUMER_HOME`, build the maven project.** - _this step will create the JAR file with all Consumer dependencies inside, in the ` $CONSUMER_HOME/bin ` directory_\r\n\r\n    mvn clean package\r\n\r\n**3. Create a config file for the Consumer Instance and provide all necessary properties.** - _Use the existing Config file `$CONSUMER_HOME`/src/main/resources/kafkaESConsumer.properties.template` as template._\r\n\r\n    cp $CONSUMER_HOME/src/main/resources/kafkaESConsumer.properties $CONSUMER_HOME/config/<consumerGroupName><topicName><PartitionNum>.properties\r\n\r\n    vi $CONSUMER_HOME/src/main/resources/<consumerGroupName><topicName><PartitionNum>.properties - Edit & provide the correct config details.\r\n\r\n\r\n_These files will be copied into the $CONSUMER_HOME/bin/classes/ dir after the build._\r\n_The details & guide for each property in the config file is given in the property file itself._\r\n\r\n\r\n**4. Start the Consumer as follows:**\r\n\r\n\r\n    cd $CONSUMER_HOME/scripts\r\n    \r\n    vi consumerNew.sh\r\n    \r\n    Provide the value for all the below variables:\r\n    \r\n    # Setup variables\r\n    #Set the full path of top directory of this kafka consumer\r\n    base_dir=\r\n    JAVA_HOME=\r\n    #User as which the Consumer Daemon has to be run\r\n    USER=\r\n\r\n    ./consumerNew.sh -p start -c $CONSUMER_HOME/config/<consumerGroupName><topicName><PartitionNum>.properties\r\n\r\n    # ' -p ' - Can take either start | stop | restart\r\n    \r\n    # ' -c ' - the config file for the consumer instance with path \r\n    # (e.g: '$CONSUMER_HOME/config/<consumerGroupName><topicName><PartitionNum>.properties')\r\n\r\n**5. Confirm the successful start of the Consumer by looking into:**\r\n\r\n_The below log file contains INFO during starting, restarting & stopping the Consumer Instance._\r\n\r\n    #'$consumerGroupName,$topic,$partition' - properties as defined in the consumer instances's config file (i.e: '<consumerGroupName><topicName><PartitionNum>.properties' in this example\r\n    \r\n    vi $CONSUMER_HOME/processLogs/<$consumerGroupName>_<$topic>_<$partition>.out\r\n\r\n_The below log file contains ERROR during starting, restarting & stopping the Consumer Instance._\r\n\r\n    #'$consumerGroupName,$topic,$partition' - properties as defined in the consumer instances's config file (i.e: '<consumerGroupName><topicName><PartitionNum>.properties' in this example\r\n\r\n    vi $CONSUMER_HOME/processLogs/<$consumerGroupName>_<$topic>_<$partition>.err\r\n\r\n**6. Monitor the processing in the log file defined by the following property in the Consumer's Respective Config file.**\r\n\r\n\r\n**7. To Stop the Consumer Instance:**\r\n\r\n    cd $CONSUMER_HOME/scripts\r\n\r\n    ./consumerNew.sh -p stop -c $CONSUMER_HOME/config/<consumerGroupName><topicName><PartitionNum>.properties\r\n\r\n**8. To Restart the Consumer Instance:**\r\n\r\n    cd $CONSUMER_HOME/scripts\r\n\r\n    ./consumerNew.sh -p restart -c $CONSUMER_HOME/config/<consumerGroupName><topicName><PartitionNum>.properties\r\n\r\n# Versions:\r\n\r\n### Kafka Version: 0.8.2.1\r\n\r\n### ElasticSearch: > 1.5.1\r\n\r\n### Scala Version for Kafka Build: 2.10.0\r\n\r\n# Configuring the Consumer Instance:\r\n\r\nThe details of each config property can be seen in the template file (below)\r\n\r\n[Config File with details about each property](https://github.com/ppine7/kafka-elasticsearch-standalone-consumer/blob/master/src/main/resources/kafkaESConsumer.properties)\r\n\r\n# Message Handler Class\r\n\r\n*  `org.elasticsearch.kafka.consumer.MessageHandler` is an Abstract class that has most of the functionality of reading data from Kafka and batch-indexing into ElasticSearch already implemented. It has one abstract method, `transformMessage()`, that can be overwritten in the concrete sub-classes to customize message transformation before posting into ES\r\n\r\n* `org.elasticsearch.kafka.consumer.messageHandlers.RawMessageStringHandler` is a simple concrete sub-class of the MessageHAndler that sends messages into ES with no additional transformation, as is, in the 'UTF-8' format\r\n\r\n* Usually, its effective to Index the message in JSON format in ElasticSearch. This can be done using a Mapper Class and transforming the message from Kafka by overriding/implementing the `transformMessage()` method. An example can be found here: `org.elasticsearch.kafka.consumer.messageHandlers.AccessLogMessageHandler`\r\n\r\n* _**Do remember to set the newly created message handler class in the `messageHandlerClass` config property of the consumer instance.**_\r\n\r\n# IndexHandler Interface and basic implementation\r\n\r\n*  `org.elasticsearch.kafka.consumer.IndexHandler` is an interface that defines two methods: getIndexName(params) and getIndexType(params). \r\n\r\n* `org.elasticsearch.kafka.consumer.BasicIndexHandler` is a simple imlementation of this interface that returnes indexName and indexType values as configured in the kafkaESConsumer.properties file. \r\n\r\n* one might want to create a custom implementation of IndexHandler if, for example, index name and type are not static for all incoming messages but depend on the event data - for example customerId, orderId, etc. In that case, pass all info that is required to perform that custom index determination logic as a Map of parameters into the getIndexName(params) and getIndexType(params) methods (or pass NULL if no such data is required)\r\n\r\n* _**Do remember to set the index handler class in the `indexHandlerClass` property in the kafkaESConsumer.properties file. By default, BasicIndexHandler is used**_\r\n\r\n# License\r\n\r\nkafka-elasticsearch-standalone-consumer\r\n\r\n\tLicensed under the Apache License, Version 2.0 (the \"License\"); you may\r\n\tnot use this file except in compliance with the License. You may obtain\r\n\ta copy of the License at\r\n\r\n\t     http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n\tUnless required by applicable law or agreed to in writing,\r\n\tsoftware distributed under the License is distributed on an\r\n\t\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n\tKIND, either express or implied.  See the License for the\r\n\tspecific language governing permissions and limitations\r\n\tunder the License.\r\n\r\n# Contributors\r\n\r\n\r\n","google":"UA-67418806-1","note":"Don't delete this file! It's used internally to help with page regeneration."}